{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, T, d_model, dim):\n",
    "        assert d_model % 2 == 0\n",
    "        super().__init__()\n",
    "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
    "        emb = torch.exp(-emb)\n",
    "        pos = torch.arange(T).float()\n",
    "        emb = pos[:, None] * emb[None, :]\n",
    "        assert list(emb.shape) == [T, d_model // 2]\n",
    "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
    "        emb = emb.view(T, d_model)\n",
    "\n",
    "        self.timembedding = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(d_model, dim),\n",
    "            Swish(),\n",
    "            nn.Linear(dim, dim),\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, t):\n",
    "        emb = self.timembedding(t)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.main.weight)\n",
    "        init.zeros_(self.main.bias)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        _, _, H, W = x.shape\n",
    "        x = F.interpolate(\n",
    "            x, scale_factor=2, mode='nearest')\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttnBlock(nn.Module):\n",
    "    def __init__(self, in_ch):\n",
    "        super().__init__()\n",
    "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
    "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in [self.proj_q, self.proj_k, self.proj_v, self.proj]:\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.proj.weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        q = self.proj_q(h)\n",
    "        k = self.proj_k(h)\n",
    "        v = self.proj_v(h)\n",
    "\n",
    "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        k = k.view(B, C, H * W)\n",
    "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
    "        assert list(w.shape) == [B, H * W, H * W]\n",
    "        w = F.softmax(w, dim=-1)\n",
    "\n",
    "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
    "        h = torch.bmm(w, v)\n",
    "        assert list(h.shape) == [B, H * W, C]\n",
    "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
    "        h = self.proj(h)\n",
    "\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=False):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.GroupNorm(32, in_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        self.temb_proj = nn.Sequential(\n",
    "            Swish(),\n",
    "            nn.Linear(tdim, out_ch),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.GroupNorm(32, out_ch),\n",
    "            Swish(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
    "        )\n",
    "        if in_ch != out_ch:\n",
    "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "        if attn:\n",
    "            self.attn = AttnBlock(out_ch)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                init.xavier_uniform_(module.weight)\n",
    "                init.zeros_(module.bias)\n",
    "        init.xavier_uniform_(self.block2[-1].weight, gain=1e-5)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        h = self.block1(x)\n",
    "        h += self.temb_proj(temb)[:, :, None, None]\n",
    "        h = self.block2(h)\n",
    "\n",
    "        h = h + self.shortcut(x)\n",
    "        h = self.attn(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, T, ch, ch_mult, attn, num_res_blocks, dropout):\n",
    "        super().__init__()\n",
    "        assert all([i < len(ch_mult) for i in attn]), 'attn index out of bound'\n",
    "        tdim = ch * 4\n",
    "        self.time_embedding = TimeEmbedding(T, ch, tdim)\n",
    "\n",
    "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.downblocks = nn.ModuleList()\n",
    "        chs = [ch]  # record output channel when dowmsample for upsample\n",
    "        now_ch = ch\n",
    "        for i, mult in enumerate(ch_mult):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks):\n",
    "                self.downblocks.append(ResBlock(\n",
    "                    in_ch=now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=(i in attn)))\n",
    "                now_ch = out_ch\n",
    "                chs.append(now_ch)\n",
    "            if i != len(ch_mult) - 1:\n",
    "                self.downblocks.append(DownSample(now_ch))\n",
    "                chs.append(now_ch)\n",
    "\n",
    "        self.middleblocks = nn.ModuleList([\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
    "            ResBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
    "        ])\n",
    "\n",
    "        self.upblocks = nn.ModuleList()\n",
    "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
    "            out_ch = ch * mult\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                self.upblocks.append(ResBlock(\n",
    "                    in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim,\n",
    "                    dropout=dropout, attn=(i in attn)))\n",
    "                now_ch = out_ch\n",
    "            if i != 0:\n",
    "                self.upblocks.append(UpSample(now_ch))\n",
    "        assert len(chs) == 0\n",
    "\n",
    "        self.tail = nn.Sequential(\n",
    "            nn.GroupNorm(32, now_ch),\n",
    "            Swish(),\n",
    "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
    "        )\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        init.xavier_uniform_(self.head.weight)\n",
    "        init.zeros_(self.head.bias)\n",
    "        init.xavier_uniform_(self.tail[-1].weight, gain=1e-5)\n",
    "        init.zeros_(self.tail[-1].bias)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Timestep embedding\n",
    "        temb = self.time_embedding(t)\n",
    "        # Downsampling\n",
    "        h = self.head(x)\n",
    "        #print(h.shape)\n",
    "        hs = [h]\n",
    "        for layer in self.downblocks:\n",
    "            h = layer(h, temb)\n",
    "            #print(h.shape)\n",
    "            hs.append(h)\n",
    "        # Middle\n",
    "        for layer in self.middleblocks:\n",
    "            h = layer(h, temb)\n",
    "        #print(h.shape)\n",
    "        # Upsampling\n",
    "        for layer in self.upblocks:\n",
    "            if isinstance(layer, ResBlock):\n",
    "                h = torch.cat([h, hs.pop()], dim=1)\n",
    "            h = layer(h, temb)\n",
    "        h = self.tail(h)\n",
    "\n",
    "        assert len(hs) == 0\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "model = UNet(\n",
    "    T=1000, ch=128, ch_mult=[1, 2, 2], attn=[1],\n",
    "    num_res_blocks=2, dropout=0.1)\n",
    "x = torch.randn(10, 3, 28, 28)\n",
    "t = torch.randint(1000, (10, ))\n",
    "y = model(x, t)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (1): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (2): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (3): UpSample(\n",
       "    (main): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (4): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (5): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (6): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (7): UpSample(\n",
       "    (main): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (8): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): AttnBlock(\n",
       "      (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (proj_q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj_k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj_v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (9): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): AttnBlock(\n",
       "      (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (proj_q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj_k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj_v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (10): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): AttnBlock(\n",
       "      (group_norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (proj_q): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj_k): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj_v): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (11): UpSample(\n",
       "    (main): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (12): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 384, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (13): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       "  (14): ResBlock(\n",
       "    (block1): Sequential(\n",
       "      (0): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (temb_proj): Sequential(\n",
       "      (0): Swish()\n",
       "      (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "    )\n",
       "    (block2): Sequential(\n",
       "      (0): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "      (1): Swish()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "    (shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (attn): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.upblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
